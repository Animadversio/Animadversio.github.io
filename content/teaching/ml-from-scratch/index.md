---
title: Machine Learning from Scratch
summary: Student-led tutorial series teaching cutting-edge ML models through hands-on implementation for neuroscience students
date: 2021-01-01
type: docs
math: true
tags:
  - Machine Learning
  - Deep Learning
  - Transformers
  - Diffusion Models
  - Tutorial Series
image:
  caption: 'Machine Learning from Scratch tutorial series'
---

## Series Information

**Program:** Machine Learning from Scratch  
**Year:** 2021  
**Organizers:** Johannes Bill, John Vastloa, Binxu Wang
**Institution:** Harvard Medical School  
**Target Audience:** Neuroscience graduate students and postdocs

## Series Objective

A student-led tutorial and seminar series designed to teach neuroscience students cutting-edge machine learning models by implementing them from scratch. The hands-on approach ensures deep understanding of both theoretical foundations and practical implementation.

## Tutorial Topics

### 1. Transformers and Large Language Models (LLMs)

**Topics Covered:**
- Natural Language Processing (NLP) fundamentals
- Attention mechanism deep dive
- Transformer architecture from first principles
- Training language models
- Usage and fine-tuning of pretrained models
- Applications beyond language: vision, audio, image generation

**Materials:**
- Interactive Jupyter/Colab notebooks
- Step-by-step implementation guides
- Detailed mathematical derivations
- Practical coding exercises

### 2. Stable Diffusion Models

**Topics Covered:**
- Principles of diffusion models
- UNet model architecture
- Contextualized word embedding
- Cross-attention mechanisms
- Autoencoder efficiency techniques
- Large-scale training strategies

**Key Concepts:**
- Forward and reverse diffusion processes
- Noise scheduling
- Conditional generation
- Image synthesis applications

### 3. Mathematical Foundation of Diffusion Generative Models

**Advanced Topics:**
- Score function and data distribution gradients
- Reversing forward diffusion process mathematically
- Score function learning techniques
- Neural network score approximation
- Advanced sampling methods (DDPM, DDIM)

**Materials:**
- Rigorous mathematical derivations
- Implementation from theoretical principles
- Performance optimization techniques
- Comparison of different sampling strategies
<!-- 
## Learning Resources

### Interactive Materials
- **Jupyter/Colab Notebooks:** Hands-on coding environments
- **GitHub Repositories:** Complete implementation code
- **Slide Decks:** Theoretical foundations and visual explanations
- **Video Recordings:** Step-by-step walkthroughs

### Reading Materials
- Curated papers from leading ML researchers
- Supplementary theoretical resources
- Implementation guides and best practices
- Recent advances in each field

## Pedagogical Approach

### Learning by Doing
- **From Scratch Implementation:** Build models without relying on high-level libraries
- **Mathematical Understanding:** Derive equations before coding
- **Iterative Learning:** Start simple, add complexity gradually
- **Debugging Practice:** Learn to identify and fix implementation issues

### Neuroscience Integration
- **Biological Inspiration:** Connect ML models to neural mechanisms
- **Research Applications:** Apply models to neuroscience data
- **Cross-Disciplinary Insights:** Bridge AI and brain science
- **Critical Analysis:** Evaluate model assumptions and limitations

## Target Skills

By completing this series, students will:
1. **Implement** state-of-the-art ML models from mathematical foundations
2. **Understand** the theoretical principles underlying modern AI systems
3. **Apply** these models to neuroscience research problems
4. **Critically evaluate** model performance and limitations
5. **Stay current** with rapidly evolving ML landscape

## Community and Collaboration

- **Peer Learning:** Student-led discussions and problem-solving
- **Expert Guidance:** Faculty and postdoc mentorship
- **Open Source:** All materials freely available
- **Reproducible Research:** Emphasis on code quality and documentation -->