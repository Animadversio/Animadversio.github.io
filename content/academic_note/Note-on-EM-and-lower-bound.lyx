#LyX file created by tex2lyx 2.3
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/binxuwang/Github/Animadversio.github.io/_academic_notes/
\textclass article
\begin_preamble
% Options for packages loaded elsewhere


%
\usepackage{iftex}
\ifPDFTeX
  \usepackage{textcomp}% provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}% this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}

\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}

\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
 % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}% disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}


\title{Note on EM algorithm and likelihood lower bound}
\author{Binxu Wang}
\date{May 13th, 2022}


\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding T1
\font_roman "lmodern" "default"
\font_sans "default" "default"
\font_typewriter "lmodern" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_title "Note on EM algorithm and likelihood lower bound"
\pdf_author "Binxu Wang"
\pdf_bookmarks 0
\pdf_bookmarksnumbered 0
\pdf_bookmarksopen 0
\pdf_bookmarksopenlevel 1
\pdf_breaklinks 0
\pdf_pdfborder 0
\pdf_colorlinks 0
\pdf_backref section
\pdf_pdfusetitle 0
\pdf_quoted_options "hidelinks,pdfcreator={LaTeX via pandoc}"
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth -\maxdimen
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
maketitle
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Motivation
\end_layout

\begin_layout Standard
How to understand EM algorithm from a theoretical perspective? This post tries to understand EM as a form of alternative ascent of a lower bound of likelihood.
\end_layout

\begin_layout Subsection
The Key Trick of EM
\end_layout

\begin_layout Standard
The key trick we need to remember is the usage of 
\series bold
Jensen Inequality
\series default
 on logarithm. So we could swap Expectation and logarithm and obtain a lower bound on likelihood. Generally, we have such inequality, given a positive function 
\begin_inset Formula \(q(z)\)
\end_inset

 that sums to 
\begin_inset Formula \(1\)
\end_inset

 (probability density), 
\begin_inset Formula \[
\log \sum_z p(z)=\log \sum_zq(z)\frac{p(z)}{q(z)}\geq \sum_zq(z) (\log p(z)-\log q(z))
\]
\end_inset

This form is particularly useful when we have a latent variable model. Since we are interested in the likelihood of data itself 
\begin_inset Formula \(\log p(x)\)
\end_inset

 the latent variable part needs to be integrated out. 
\begin_inset Formula \[
\log \int_zp(x|z)p(z) \geq \int_z q(z)\log\frac{p(x|z)p(z)}{q(z)}\\
=\mathbb E_{z\sim q(z)} [\log p(x|z) + \log p(z) - \log q(z)]
\]
\end_inset

This tricky is the foundation of variational inference. However, this still feels distant from the common formulation of EM. The distinctive feature of EM is that it could optimize 
\begin_inset Formula \(q(z)\)
\end_inset

 in one step: by setting 
\begin_inset Formula \(q(z)=p(z|x)\)
\end_inset

 , then without any optimization iteration, you could tighten the bound and optimize the probe distribution 
\begin_inset Formula \(q\)
\end_inset

 .
\end_layout

\begin_layout Subsection
Example: Gaussian Mixture
\end_layout

\begin_layout Standard
To make it concrete, let's take a simple example, Gaussian mixture models let observed variable 
\begin_inset Formula \(X\)
\end_inset

, discrete latent variable 
\begin_inset Formula \(Z\)
\end_inset

 and parameters 
\begin_inset Formula \(\Theta\)
\end_inset

.
\end_layout

\begin_layout Standard

\begin_inset Formula \[
P(Z=k;\Theta)=\pi_k\\
P(X\mid Z=k;\Theta)=\mathcal N(\mu_k,\Sigma_k)
\]
\end_inset


\end_layout

\begin_layout Standard
Then the likelihood function is the summation over dataset 
\begin_inset Formula \(x\in D\)
\end_inset


\end_layout

\begin_layout Standard

\begin_inset Formula \[
\log\mathcal L(\Theta)=\sum_i \log P(x_i;\Theta)=\sum_i \log \sum_k P(x_i\mid z=k;\Theta)P(z=k;\Theta)
\]
\end_inset


\end_layout

\begin_layout Standard
As a trick, we use an arbitrary distribution over latent variables 
\begin_inset Formula \(q(z)\)
\end_inset

 to probe it
\end_layout

\begin_layout Standard

\begin_inset Formula \[
\log\mathcal L(\Theta)=\sum_i \log \sum_k q(z=k)\frac{ P(x_i\mid z=k;\Theta)P(z=k;\Theta)}{q(z=k)}\\
\geq\sum_i \sum_k q(z=k) \log \frac{ P(x_i\mid z=k;\Theta)P(z=k;\Theta)}{q(z=k)}
\]
\end_inset


\end_layout

\begin_layout Standard
In the 2nd line, the swap of 
\begin_inset Formula \(\log\)
\end_inset

 and 
\begin_inset Formula \(\sum\)
\end_inset

 comes from Jensen Inequality, with 
\begin_inset Formula \(\log\)
\end_inset

 a concave function and 
\begin_inset Formula \(q(z)\)
\end_inset

 probability summing to 
\begin_inset Formula \(1\)
\end_inset

. Since 
\begin_inset Formula \(z\)
\end_inset

 is discrete, function 
\begin_inset Formula \(q(z)\)
\end_inset

 is just a vector 
\begin_inset Formula \(q\)
\end_inset

.
\end_layout

\begin_layout Standard
Let's call the lower bound 
\begin_inset Formula \(J(q,\theta)\)
\end_inset

, then the EM algorithm is maximizing it alternatively,
\end_layout

\begin_deeper
\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
tightlist
\end_layout

\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
E step 
\begin_inset Formula \(q^{(t)}\gets \arg\max_q J(q,\Theta^{(t)})\)
\end_inset

 
\end_layout

\begin_layout Itemize
M step 
\begin_inset Formula \(\Theta^{(t+1)}\gets \arg\max_\Theta J(q^{(t)},\Theta)\)
\end_inset

 
\end_layout

\begin_layout Standard
In classic formulation, E step estimate the posterior of 
\begin_inset Formula \(Z\)
\end_inset

 given 
\begin_inset Formula \(X\)
\end_inset

 and current 
\begin_inset Formula \(\Theta\)
\end_inset

. To see the connection, I claim, 
\begin_inset Formula \(q(z)=p(z\mid x;\Theta^{(t)})\)
\end_inset

 solve the maximization problem 
\begin_inset Formula \(\arg\max_q J(q,\Theta^{(t)})\)
\end_inset

.
\end_layout

\begin_layout Standard
To prove this, we show that the posterior makes the aforementioned bound tight. When 
\begin_inset Formula \(q(z)=p(z\mid x;\Theta)\\\)
\end_inset


\end_layout

\begin_layout Standard

\begin_inset Formula \[
\log p(x;\Theta) = \log \sum_z p(x,z;\Theta) \\
\geq \sum_z q(z)\log \frac{p(x,z;\Theta)}{q(z)}\\
=\sum_z p(z\mid x;\Theta) \log \frac{p(x,z;\Theta)}{p(z\mid x;\Theta)} \\
=\sum_z p(z\mid x;\Theta) \log p(x;\Theta)\\
=\log p(x;\Theta)
\]
\end_inset


\end_layout

\begin_layout Standard
which is tight. This shows that 
\begin_inset Formula \(p(z\mid x;\Theta)=\arg\max_q J(q,\Theta^{(t)})\)
\end_inset

. As we said, this maximization could be done in closed form without iteration.
\end_layout

\begin_layout Standard
Similarly in M step, retaining only terms related to 
\begin_inset Formula \(\Theta\)
\end_inset


\end_layout

\begin_layout Standard

\begin_inset Formula \[
J(q^{(t)},\Theta)\\
=\sum_i \sum_k q_i(z=k) (\log P(x_i\mid z=k;\Theta)+\log P(z=k;\Theta)-\log q_i(z=k))\\
=\sum_i \sum_k q_i(z=k) (\log P(x_i\mid z=k;\Theta)+\log P(z=k;\Theta))+const 
\]
\end_inset


\end_layout

\begin_layout Standard
Which is a weighted MLE, for basic distributions like Gaussian it's also solvable in one step in closed form.
\end_layout

\begin_layout Standard
Using Gaussian as example, abreviating 
\begin_inset Formula \(q_i(z_k)=q_{ik}\)
\end_inset


\end_layout

\begin_layout Standard

\begin_inset Formula \[
LHS=\sum_i \sum_k q_{ik}(-\frac 12\log \det\Sigma_k  -\frac 12(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k) + \log \pi_k)+const
\]
\end_inset


\end_layout

\begin_layout Standard
Optimizing w.r.t. 
\begin_inset Formula \(\Sigma_k,\mu_k,\pi_k\)
\end_inset

 all have closed form solution. No gradient iteration is needed.
\end_layout

\begin_layout Standard
All in all, EM solve subproblem of part of variables alternatively, but each subproblem could be maximized to global maxima without iteration.
\end_layout

\begin_layout Subsection
Compare EM and Gradient Descent
\end_layout

\begin_deeper
\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
tightlist
\end_layout

\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
One way to think of the EM algorithm is that it's doing coordinate ascent/descent on a surrogate, i.e.
\begin_inset space ~

\end_inset

a 
\series bold
lower bound of likelihood
\series default
. (your textbook also showed this point on P396) It optimizes one set of variables and then the other set of variables, alternatively. However, 
\series bold
for each set of variables the optimization problem has a closed-form global maximum
\series default
, which could be optimized without iteration. 
\end_layout

\begin_layout Itemize
In contrast, gradient descent optimizes the likelihood directly. It optimizes all variables jointly but always takes local optimization steps. Even for part of the quadratic problem grad descent could be slow to converge, not to mention hard multi-modal problems. 
\end_layout

\begin_layout Standard
\align center

\begin_inset CommandInset line
LatexCommand rule
width "50line%"
height "0.5pt"

\end_inset


\end_layout

\begin_layout Subsection
Issues of EM
\end_layout

\begin_layout Standard
That being said, EM has its own problem * Note that, EM directly optimizes the lower bound of 
\begin_inset Formula \(\mathcal L(\Theta)\)
\end_inset

. the bound is tight only when 
\begin_inset Formula \(q(z)=P(z\mid x;\Theta)\)
\end_inset

, so when 
\begin_inset Formula \(z\)
\end_inset

 itself has a complex posterior distribution (not a simple discrete distr.), then parametrizing it and optimizing it becomes tricky. For example, one could use a Gaussian distribution to approximate the posterior, but when this approximation is not exact, EM iteration is not guaranteed to increase the data likelihood.
\end_layout

\begin_layout Standard

\series bold
Reference
\series default
 * 
\begin_inset CommandInset href
LatexCommand href
name "Short Lecture note from Princeton"
target "https://www.cs.princeton.edu/courses/archive/spring08/cos424/scribe_notes/0311b"
literal "false"

\end_inset

which I found most illuminating! * 
\begin_inset CommandInset href
LatexCommand href
name "Lecture slides from U Toronto"
target "https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/lec15_16_handout.pdf"
literal "false"

\end_inset

longer but quite hand holding * 
\begin_inset CommandInset href
LatexCommand href
name "EM and variational inference"
target "https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/"
literal "false"

\end_inset

* Bishop's 
\begin_inset CommandInset href
LatexCommand href
name "PRML"
target "https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/"
literal "false"

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
tightlist
\end_layout

\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
This post is adapted from 
\begin_inset CommandInset href
LatexCommand href
name "my   answer in math exchange"
target "https://math.stackexchange.com/questions/4444592/em-algorithm-vs-gradient-descent/4450007\#4450007"
literal "false"

\end_inset


\end_layout

\end_body
\end_document
